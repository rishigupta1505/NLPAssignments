{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPRishiGupta.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODSdJ5cLf7MJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "outputId": "6acb9c67-957b-4414-b9c5-1f3239468c8b"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNLdPV31izIh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7960496a-a626-41cf-862b-6478f8ac29cc"
      },
      "source": [
        "######### Word Tokenize Example ##################\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Rishi Gupta NLP Assignment. Completed\"\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Rishi', 'Gupta', 'NLP', 'Assignment', '.', 'Completed']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZANklvOHjUly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c69aa3d2-a60e-4404-99e2-5f513f397948"
      },
      "source": [
        "######### Sentence Tokenize Example ##################\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Rishi Gupta NLP Assignment. Completed!\"\n",
        "print(sent_tokenize(text))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Rishi Gupta NLP Assignment.', 'Completed!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbO19egBjqE9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "fd12f911-3ca3-46c7-a166-f5cb7529b624"
      },
      "source": [
        "####### Stop Words Removal ################\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "example_sent = \"Christ, he thinks, by my age I ought to know. You don’t get on by being original. You don’t get on by being bright. You don’t get on by being strong. You get on by being a subtle crook; somehow he thinks that’s what Norris is, and he feels an irrational dislike taking root, and he tries to dismiss it, because he prefers his dislikes rational, but after all, these circumstances are extreme, the cardinal in the mud, the humiliating tussle to get him back in the saddle, the talking, talking on the barge, and worse, the talking, talking on his knees, as if Wolsey’s unravelling, in a great unweaving of scarlet thread that might lead you back into a scarlet labyrinth, with a dying monster at its heart.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "\n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence) "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Christ', ',', 'he', 'thinks', ',', 'by', 'my', 'age', 'I', 'ought', 'to', 'know', '.', 'You', 'don', '’', 't', 'get', 'on', 'by', 'being', 'original', '.', 'You', 'don', '’', 't', 'get', 'on', 'by', 'being', 'bright', '.', 'You', 'don', '’', 't', 'get', 'on', 'by', 'being', 'strong', '.', 'You', 'get', 'on', 'by', 'being', 'a', 'subtle', 'crook', ';', 'somehow', 'he', 'thinks', 'that', '’', 's', 'what', 'Norris', 'is', ',', 'and', 'he', 'feels', 'an', 'irrational', 'dislike', 'taking', 'root', ',', 'and', 'he', 'tries', 'to', 'dismiss', 'it', ',', 'because', 'he', 'prefers', 'his', 'dislikes', 'rational', ',', 'but', 'after', 'all', ',', 'these', 'circumstances', 'are', 'extreme', ',', 'the', 'cardinal', 'in', 'the', 'mud', ',', 'the', 'humiliating', 'tussle', 'to', 'get', 'him', 'back', 'in', 'the', 'saddle', ',', 'the', 'talking', ',', 'talking', 'on', 'the', 'barge', ',', 'and', 'worse', ',', 'the', 'talking', ',', 'talking', 'on', 'his', 'knees', ',', 'as', 'if', 'Wolsey', '’', 's', 'unravelling', ',', 'in', 'a', 'great', 'unweaving', 'of', 'scarlet', 'thread', 'that', 'might', 'lead', 'you', 'back', 'into', 'a', 'scarlet', 'labyrinth', ',', 'with', 'a', 'dying', 'monster', 'at', 'its', 'heart', '.']\n",
            "['Christ', ',', 'thinks', ',', 'age', 'I', 'ought', 'know', '.', 'You', '’', 'get', 'original', '.', 'You', '’', 'get', 'bright', '.', 'You', '’', 'get', 'strong', '.', 'You', 'get', 'subtle', 'crook', ';', 'somehow', 'thinks', '’', 'Norris', ',', 'feels', 'irrational', 'dislike', 'taking', 'root', ',', 'tries', 'dismiss', ',', 'prefers', 'dislikes', 'rational', ',', ',', 'circumstances', 'extreme', ',', 'cardinal', 'mud', ',', 'humiliating', 'tussle', 'get', 'back', 'saddle', ',', 'talking', ',', 'talking', 'barge', ',', 'worse', ',', 'talking', ',', 'talking', 'knees', ',', 'Wolsey', '’', 'unravelling', ',', 'great', 'unweaving', 'scarlet', 'thread', 'might', 'lead', 'back', 'scarlet', 'labyrinth', ',', 'dying', 'monster', 'heart', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZsl8NFjlypt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "50199c10-d618-4613-fc65-fae31b6223eb"
      },
      "source": [
        "####### Stemming ################\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "for w in nltk_tokens:\n",
        "       print(\"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual: It  Stem: It\n",
            "Actual: originated  Stem: origin\n",
            "Actual: from  Stem: from\n",
            "Actual: the  Stem: the\n",
            "Actual: idea  Stem: idea\n",
            "Actual: that  Stem: that\n",
            "Actual: there  Stem: there\n",
            "Actual: are  Stem: are\n",
            "Actual: readers  Stem: reader\n",
            "Actual: who  Stem: who\n",
            "Actual: prefer  Stem: prefer\n",
            "Actual: learning  Stem: learn\n",
            "Actual: new  Stem: new\n",
            "Actual: skills  Stem: skill\n",
            "Actual: from  Stem: from\n",
            "Actual: the  Stem: the\n",
            "Actual: comforts  Stem: comfort\n",
            "Actual: of  Stem: of\n",
            "Actual: their  Stem: their\n",
            "Actual: drawing  Stem: draw\n",
            "Actual: rooms  Stem: room\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3Efk-lbmipI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "6a9d2c0a-2b50-4d05-bd54-e4104d897b52"
      },
      "source": [
        "####### Lemmatization ################\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "for w in nltk_tokens:\n",
        "       print(\"Actual: %s  Lemma: %s\"  % (w,wordnet_lemmatizer.lemmatize(w)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual: It  Lemma: It\n",
            "Actual: originated  Lemma: originated\n",
            "Actual: from  Lemma: from\n",
            "Actual: the  Lemma: the\n",
            "Actual: idea  Lemma: idea\n",
            "Actual: that  Lemma: that\n",
            "Actual: there  Lemma: there\n",
            "Actual: are  Lemma: are\n",
            "Actual: readers  Lemma: reader\n",
            "Actual: who  Lemma: who\n",
            "Actual: prefer  Lemma: prefer\n",
            "Actual: learning  Lemma: learning\n",
            "Actual: new  Lemma: new\n",
            "Actual: skills  Lemma: skill\n",
            "Actual: from  Lemma: from\n",
            "Actual: the  Lemma: the\n",
            "Actual: comforts  Lemma: comfort\n",
            "Actual: of  Lemma: of\n",
            "Actual: their  Lemma: their\n",
            "Actual: drawing  Lemma: drawing\n",
            "Actual: rooms  Lemma: room\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}